defaults:
  - override hydra/job_logging: none

seed: 0

trainer:
  devices: 1
  num_nodes: 1
  # For available strategies look at the PyTorch Lightning docs.
  strategy: ddp_find_unused_parameters_true
  # Look at the PyTorch Lightning docs for available precision.
  precision: bf16-mixed

  # logging settings
  wandb: true
  wandb_project: minVAE
  log_every_n_steps: 10
  gradnorm_logging: false
  rich_print: true

  # checkpointing settings
  checkpoint_save_dir: null
  checkpoint_name: null
  save_loc: null # no need to specify this.

model:
  compile: true
  config:
    # num channels in input and output
    in_channels: 3
    out_channels: 3
    # base number of channels in the encoder and decoder.
    channels: 32
    # how the channels change in the encoder and decoder.
    channels_mult: [2, 4, 4]
    # number of residual blocks in the upsampling (dec.) and downsampling (enc.) path.
    num_res_blocks: 2
    # resolution at which to apply self-attention in the encoder downsampling.
    attn_resolutions: [32]
    dropout: 0.0
    # input resolution
    resolution: 64
    # latent size, for VAE the encoder predicts 2 * latent_size.
    # representing the mean and logvar of the latent distribution.
    z_channels: 16
    # spatial compression controls how much to
    spatial_compression: 4
    # wavelet settings
    wavelet: haar # supports the wavelets from PyWavelets
    maxlevel: 2 # number of levels to apply wavelet transform
    

dataset:
  name: celeba
  num_classes: 0
  data_path: null
  image_size: 64
  max_crop_size: 64
  augmentations:
    horizontal_flip: true

  # dataloader settings
  batch_size: 128
  num_workers: 8
  val_batch_size: 32
  val_num_workers: 4
  pin_memory: true

  # distributed sampler
  use_distributed_sampler: false

loss:
  # reconstruction loss
  recon_loss: l2
  # kl loss weight
  kl_weight: 1.0

  perceptual_loss:
    enable: false
    type: lpips
    model_name: vgg
    weight: 1.0

  adversarial_loss:
    enable: false
    weight: 1.0
    discriminator: small
    loss_type: bce # bce or hinge

optimizer:
  # general settings
  total_steps: 100_000
  optimizer: adamw
  lr: 1e-4
  weight_decay: 1e-3
  betas: [0.9, 0.95]
  grad_clip: null
  ema: null
  accumulate_grad_batches: 1 # this is not implemented.
  overfit_batches: 0.0 # for debugging purposes, [0.0, 1.0]
  # shampoo specific settings
  preconditioning_frequency: null
  max_preconditioner_dim: null
  start_preconditioning_step: null
  # scheduler settings
  schedule: cosine
  warmup_steps: 500
  warmup_lr: 1e-6
  min_lr: 0.0
  power: null
  # discriminator optimizer settings
  discriminator:
    head_only: true
    optimizer: adamw
    lr: 1e-4
    weight_decay: 1e-3
    betas: [0.9, 0.95]
    discriminator_warmup: 0